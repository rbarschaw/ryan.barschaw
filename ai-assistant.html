<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Partner Center AI Assistant – Case Study – Ryan Barschaw</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="description" content="How I led the Partner Center AI Assistant from blank page to production: content safety, responsible AI governance, guardrails, and lessons learned." />
    
    <!-- Open Graph / Social -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://ryan.barschaw.com/ai-assistant.html" />
    <meta property="og:title" content="Partner Center AI Assistant – Case Study – Ryan Barschaw" />
    <meta property="og:description" content="How I led the Partner Center AI Assistant from blank page to production: content safety, responsible AI governance, guardrails, and lessons learned." />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Partner Center AI Assistant – Case Study – Ryan Barschaw" />
    <meta name="twitter:description" content="How I led the Partner Center AI Assistant from blank page to production: content safety, responsible AI governance, guardrails, and lessons learned." />
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <link rel="alternate icon" href="/favicon.ico" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=DM+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <!-- Skip Link for Accessibility -->
    <a href="#main-content" class="skip-link">Skip to main content</a>
    
    <nav role="navigation" aria-label="Main navigation">
      <div class="nav-inner">
        <a href="/" class="logo">ryan<span>.</span>barschaw</a>
        <div class="nav-links" id="navLinks">
          <a href="/">Home</a>
          <a href="/#work">Work</a>
          <a href="/#contact">Contact</a>
        </div>
        <button class="menu-toggle" id="menuToggle" aria-label="Toggle menu">☰</button>
      </div>
    </nav>

    <main id="main-content">
    <section class="case-hero">
      <div class="container container--narrow">
        <div class="breadcrumb">
          <a href="/">Home</a>
          <span>→</span>
          <a href="/#work">Work</a>
          <span>→</span>
          <span>AI Assistant</span>
        </div>
        <div class="case-tag">Featured Case Study</div>
        <h1>Partner Center AI Assistant</h1>
        <p class="case-subtitle">
          From blank page to a production LLM experience that 400K+ partners actually use. Built with guardrails, content safety, and responsible AI governance from day one.
        </p>
        <div class="case-meta">
          <div class="meta-item">
            <span class="meta-label">Role</span>
            <span class="meta-value">Principal Product Manager</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Company</span>
            <span class="meta-value">Microsoft Partner Center</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Timeline</span>
            <span class="meta-value">2023 – 2025</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Recognition</span>
            <span class="meta-value">C+E AI Contributor Badge</span>
          </div>
        </div>
      </div>
    </section>

    <section class="case-content">
      <div class="container container--narrow">

        <h2>The Problem</h2>
        <p>
          Partner Center is where Microsoft's 400,000+ cloud partners manage their business: offers, incentives, billing, support, and more. It's powerful, but it's also a maze. Multiple surfaces, changing policies, and business rules buried across dozens of documentation articles.
        </p>
        <p>
          Partners were asking the same questions again and again:
        </p>
        <ul>
          <li>"Why is this offer unavailable in my region?"</li>
          <li>"Why didn't my incentive pay out this quarter?"</li>
          <li>"What do I need to do to fix my account verification?"</li>
          <li>"Can I sell this Azure plan to customers in Germany?"</li>
          <li>"Why am I not seeing my co-sell opportunity in the pipeline?"</li>
        </ul>
        <p>
          Support could answer these questions, but it didn't scale. Every answer required a human to look up the partner's account data, cross-reference policies, and explain what was happening. Documentation alone wasn't enough—partners needed answers grounded in <em>their</em> data, not generic articles.
        </p>

        <h3>The Scale of the Challenge</h3>
        <ul>
          <li><strong>400,000+ partners</strong> across every geography and time zone</li>
          <li><strong>100+ offer types</strong> with different eligibility rules, pricing, and regional availability</li>
          <li><strong>Dozens of incentive programs</strong> with complex calculation logic and payout schedules</li>
          <li><strong>Regulatory complexity</strong> varying by country, partner type, and customer segment</li>
          <li><strong>Multiple systems</strong> that each held part of the truth (billing, identity, offers, incentives)</li>
        </ul>

        <div class="callout">
          <div class="callout-title">The core insight</div>
          Partners didn't just want information—they wanted <strong>contextual answers</strong> about their specific account, offers, and situation. A generic chatbot that says "check the documentation" is worse than no chatbot at all. We needed to ground every answer in real account data.
        </div>

        <h2>My Approach</h2>
        <p>
          I framed the problem as: <strong>"How do we give partners a single conversational surface that explains what's happening in their data, not just in generic docs, and helps them take the right next step?"</strong>
        </p>

        <h3>North Star Principles</h3>
        <ol>
          <li><strong>Ground answers in real data.</strong> Account, offer, and billing data—not just generic text. The assistant should know what offers a partner is eligible for, why an incentive didn't pay out, and what's blocking their account.</li>
          <li><strong>Make it a front door, not a toy.</strong> Integrate into existing workflows, not bolted on the side. The assistant should live where partners already work, not require them to go somewhere new.</li>
          <li><strong>Design for governance from day one.</strong> Logs, evaluations, and clear guardrails baked into the architecture. We couldn't afford to ship something and then figure out safety later.</li>
          <li><strong>Crawl-walk-run.</strong> Start with narrow, high-value scenarios. Learn from real usage. Expand gradually as we build confidence.</li>
        </ol>

        <h3>How I Worked</h3>
        <ul>
          <li><strong>Co-created scenarios with support and field teams.</strong> Spent weeks with partner development managers and support engineers to understand the real questions partners were asking. Built a prioritized backlog based on frequency and value.</li>
          <li><strong>Wrote simple, concrete one-pagers.</strong> Each scenario got a one-page spec that engineering, data science, and stakeholders could all read the same way. No 50-page PRDs. Clear problem statement, success criteria, and acceptance tests.</li>
          <li><strong>Defined evaluation criteria before building.</strong> For each scenario, we defined what a "good" answer looked like. These became our evals and our quality bar.</li>
          <li><strong>Set up weekly reviews with engineering and science leads.</strong> Fast feedback loops. Unblock issues in days, not weeks. Adjust course based on real feedback, not speculation.</li>
          <li><strong>Partnered with legal, privacy, and security early.</strong> Got them involved during design, not after launch. This avoided late-stage surprises and built trust.</li>
        </ul>

        <h3>Example: Incentive Questions</h3>
        <p>
          One of our highest-value scenarios was incentive questions. Partners would ask "Why didn't I get paid?" and previously had to wait for support to investigate. With the AI Assistant:
        </p>
        <ul>
          <li>The assistant retrieves the partner's incentive data for the relevant period</li>
          <li>It checks calculation logic, eligibility rules, and payout status</li>
          <li>It explains specifically why the incentive did or didn't pay out (e.g., "You didn't meet the threshold for new customer adds")</li>
          <li>It suggests next steps if applicable (e.g., "You can appeal by...")</li>
        </ul>
        <p>
          This turned a multi-day support ticket into a 30-second conversation.
        </p>

        <h2>Architecture</h2>
        <p>
          The assistant sits on top of existing Partner Center APIs and data platforms. My job was to make sure the architecture supported grounded answers, explainability, and future extensibility.
        </p>

        <div class="architecture-box">
          <pre>
┌─────────────────────────────────────────────────────────────────┐
│                        User Query                               │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Orchestration Layer                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │   Intent     │  │   Grounding  │  │   Safety     │          │
│  │   Routing    │  │   & Context  │  │   Filters    │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└─────────────────────────────────────────────────────────────────┘
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐
│  Partner APIs    │ │  Offer Catalog   │ │  Incentive Data  │
│  (Account, MPN)  │ │  (Pricing, SKUs) │ │  (Earnings, Inv) │
└──────────────────┘ └──────────────────┘ └──────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                     LLM + Reasoning                             │
│            (Azure OpenAI / Copilot Runtime)                     │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Response + Actions                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │   Grounded   │  │   Plugins    │  │   Telemetry  │          │
│  │   Answer     │  │   (Actions)  │  │   & Logging  │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└─────────────────────────────────────────────────────────────────┘
          </pre>
        </div>

        <h3>Key Architectural Decisions</h3>
        <ul>
          <li><strong>Retrieval-augmented generation (RAG)</strong> to pull the right partner, offer, and incentive data per query. The LLM never hallucinates account data because it's always retrieved fresh from source systems.</li>
          <li><strong>Plugin architecture</strong> so the assistant could take actions (open a page, start a workflow, file a ticket) instead of just answering questions. Partners could say "help me fix this" and the assistant would kick off the right process.</li>
          <li><strong>Detailed telemetry at each step</strong> (retrieval, reasoning, action) so we could debug and improve real conversations. Every interaction was logged with full context for later analysis.</li>
          <li><strong>Tenant isolation</strong> to ensure partners only see their own data, never anyone else's. This was non-negotiable and built into the retrieval layer.</li>
          <li><strong>Graceful degradation</strong> for when systems are unavailable or data is incomplete. The assistant would explain what it couldn't access and suggest alternatives.</li>
        </ul>

        <h3>Tradeoffs We Made</h3>
        <ul>
          <li><strong>Accuracy over speed.</strong> We chose to make extra API calls to verify data rather than rely on cached or stale information. This added latency but dramatically improved trust.</li>
          <li><strong>Narrow and deep over broad and shallow.</strong> We launched with fewer scenarios but made each one really good, rather than trying to answer everything poorly.</li>
          <li><strong>Explainability over terseness.</strong> The assistant always explained its reasoning, even when that made responses longer. Partners needed to understand <em>why</em>, not just what.</li>
        </ul>

        <h2>Governance & Responsible AI</h2>
        <p>
          Because this assistant dealt with production account data and incentives (real money), Responsible AI and compliance were non-negotiable. I served as the governance lead for AI systems in Partner Center.
        </p>

        <h3>The Four Pillars of Content Safety</h3>
        <p>
          Every AI response had to pass through guardrails across four dimensions:
        </p>
        <div class="stats-grid">
          <div class="stat-card">
            <div class="value" style="font-size: 1.5rem;">Policy</div>
            <div class="label">Responses aligned to business rules, regional requirements, and partner agreements</div>
          </div>
          <div class="stat-card">
            <div class="value" style="font-size: 1.5rem;">Privacy</div>
            <div class="label">Tenant isolation, data minimization, no cross-partner leakage</div>
          </div>
          <div class="stat-card">
            <div class="value" style="font-size: 1.5rem;">Security</div>
            <div class="label">Authentication, authorization, audit logging at every step</div>
          </div>
          <div class="stat-card">
            <div class="value" style="font-size: 1.5rem;">Safety</div>
            <div class="label">Content filtering, hallucination prevention, harmful output blocking</div>
          </div>
        </div>
        <p>
          This four-pillar framework became our standard for evaluating any new AI capability. Before a scenario could ship, it had to demonstrate compliance across all four dimensions with documented mitigations for identified risks.
        </p>

        <h3>Content Filtering & Guardrails</h3>
        <ul>
          <li><strong>Input filtering:</strong> Detected and blocked prompt injection attempts, adversarial inputs, and out-of-scope queries before they reached the LLM</li>
          <li><strong>Output filtering:</strong> Scanned every response for harmful content, PII leakage, and policy violations before returning to the user</li>
          <li><strong>Grounding verification:</strong> Cross-checked LLM claims against source data to prevent hallucination of account details or financial information</li>
          <li><strong>Confidence thresholds:</strong> Low-confidence responses triggered human handoff rather than guessing</li>
        </ul>

        <h3>What Governance Actually Looked Like</h3>
        <ul>
          <li><strong>Mapped every scenario to Microsoft's Responsible AI Standard (RAIS).</strong> Each scenario had explicit documentation of risks, mitigations, and acceptance criteria aligned to RAIS principles.</li>
          <li><strong>Built evaluation sets before launch.</strong> For each scenario, we created test cases covering normal inputs, edge cases, adversarial inputs, and failure modes. These ran automatically on every build.</li>
          <li><strong>Spot-check process for production.</strong> A rotating sample of real conversations was reviewed by humans to catch issues the automated evals might miss.</li>
          <li><strong>Clear escalation paths.</strong> When the assistant couldn't answer confidently, it said so and offered to connect the partner with human support. No bluffing.</li>
          <li><strong>Incident response playbook.</strong> We had documented procedures for what to do if the assistant gave harmful or incorrect advice. Never used it, but we were ready.</li>
        </ul>

        <h3>Working with Legal and Privacy</h3>
        <p>
          AI projects often fail because legal and privacy teams get surprised at the end. I brought them in during design:
        </p>
        <ul>
          <li>Shared architecture diagrams and data flows early, before code was written</li>
          <li>Documented what data the LLM could and couldn't see</li>
          <li>Explained how tenant isolation worked and why data couldn't leak between partners</li>
          <li>Walked through example conversations to show what the assistant would actually say</li>
        </ul>
        <p>
          By the time we were ready to launch, legal and privacy weren't blockers—they were advocates.
        </p>

        <div class="callout">
          <div class="callout-title">Recognition</div>
          I was recognized with the <strong>C+E AI Contributor badge</strong> for driving responsible AI adoption and helping other teams apply similar governance patterns. Several other teams used our governance framework as a template.
        </div>

        <h2>Outcomes</h2>

        <div class="stats-grid">
          <div class="stat-card">
            <div class="value">400K+</div>
            <div class="label">Partners with access</div>
          </div>
          <div class="stat-card">
            <div class="value">24/7</div>
            <div class="label">Always-on support</div>
          </div>
          <div class="stat-card">
            <div class="value">↓</div>
            <div class="label">Reduced support burden</div>
          </div>
        </div>

        <p>
          The result was a production AI assistant that partners actually used. It didn't replace every workflow, but it made it much faster to understand "what is going on with my account?" and "what should I do next?".
        </p>

        <h3>What Changed for Partners</h3>
        <ul>
          <li><strong>Instant answers to incentive questions</strong> that previously required multi-day support tickets</li>
          <li><strong>Self-service for offer eligibility</strong>—partners could understand why they could or couldn't sell something in a specific region</li>
          <li><strong>Account status clarity</strong>—no more guessing why something was blocked or what action was needed</li>
          <li><strong>Available 24/7</strong> regardless of time zone or support queue length</li>
        </ul>

        <h3>What Changed for the Organization</h3>
        <ul>
          <li><strong>Reduced support escalations</strong> for routine questions that the assistant could handle</li>
          <li><strong>Better data on partner pain points</strong>—conversation logs revealed issues we didn't know existed</li>
          <li><strong>A reusable platform</strong> that other teams could extend with their own scenarios and data sources</li>
          <li><strong>A governance template</strong> that other AI projects in the org adopted</li>
        </ul>

        <p>
          More importantly, we built a foundation that could be extended—new plugins, new data sources, new scenarios—without re-architecting the whole system. The assistant is still growing.
        </p>

        <h2>What I Learned</h2>

        <div class="lessons-grid">
          <div class="lesson-card">
            <h4>Grounding > prompts</h4>
            <p>Clever prompts don't matter if the data is wrong or inaccessible. The hard work is in retrieval, context, and data quality.</p>
          </div>
          <div class="lesson-card">
            <h4>Simple docs win</h4>
            <p>One-pagers that everyone can read the same way beat intricate diagrams when you're coordinating across teams and time zones.</p>
          </div>
          <div class="lesson-card">
            <h4>Governance is architecture</h4>
            <p>Responsible AI is easiest when it's built into the architecture, telemetry, and rollout plan—not bolted on at the end.</p>
          </div>
          <div class="lesson-card">
            <h4>Ship, then iterate</h4>
            <p>We learned more in the first week of production than months of planning. Crawl-walk-run works.</p>
          </div>
        </div>

        <p>
          This is the kind of work I want to keep doing: messy, cross-team AI problems where you have to think about systems, people, and risk all at once—and then ship something real.
        </p>

        <div class="case-cta">
          <h3>Want to go deeper?</h3>
          <p>Happy to discuss the technical details, governance approach, or lessons learned.</p>
          <a href="mailto:ryan@barschaw.com?subject=AI%20Assistant%20Case%20Study" class="btn btn-primary">Get in Touch</a>
          <a href="/" class="btn btn-secondary">Back to Home</a>
        </div>

      </div>
    </section>
    </main>

    <footer>
      <div class="container">
        <p>© <span id="year"></span> Ryan Barschaw</p>
      </div>
    </footer>

    <script>
      document.getElementById('year').textContent = new Date().getFullYear();
      
      // Mobile menu toggle
      document.getElementById('menuToggle').addEventListener('click', () => {
        document.getElementById('navLinks').classList.toggle('active');
      });
    </script>

  </body>
</html>
